{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho de Inteligência Artificial - 2019/2\n",
    "\n",
    "Gustavo Eugênio de Souza Moraes RA: 620238<br>\n",
    "Gabriel Givigi Oliveira RA: 760924"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas utilizadas e variáveis globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv # for reading tsv files and structuring\n",
    "import math # for mathematical functions\n",
    "import numpy as np # for vector stuff\n",
    "import matplotlib.pyplot as plt # for plotting graphs\n",
    "from random import randint # for generating random colorse\n",
    "#from sklearn.metrics.cluster import adjusted_rand_score # for rand index measure\n",
    "\n",
    "dataset1 = read_csv(\"datasets/c2ds1-2sp.txt\", sep=\"\\t\") # Lê o arquivo de dados\n",
    "dataset2 = read_csv(\"datasets/c2ds3-2g.txt\", sep=\"\\t\") # Lê o arquivo de dados\n",
    "dataset3 = read_csv(\"datasets/monkey.txt\", sep=\"\\t\") # Lê o arquivo de dados\n",
    "\n",
    "# funcção de distância euclidiana\n",
    "def myeuclidean(p, q):\n",
    "    return math.sqrt((p['d1']-q['d1'])**2+(p['d2']-q['d2'])**2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo K-Médias\n",
    "\n",
    "Aqui temos nossa implementação do algoritmo K-Médias, feita sem a necessidade de consulta apenas seguindo as etapas do algoritmo demonstrads em aula. O código se divide em 4 etapas, sendo elas:\n",
    "1. <b>INICIALIZAÇÃO</b><br>\n",
    "    &ensp; A primeira etapa consiste em definir os k-centróides iniciais necessários para o funcionamento do algortimo. Nossa solução foi definir como centróides iniciais os k primeiros valores do <em>dataframe</em> formado. \n",
    "2. <b>(RE)CALCULAR CENTRÓIDE</b><br>\n",
    "    &ensp; O cálculo do centróide é feito de maneira simples, nós montamos uma <em>view</em> do <em>dataframe</em> filtrando pela coluna <em>\"cluster_id\"</em>. Em seguida com a função mean() tiramos a média aritmética de cada grupo marcado pelo ID do cluster, e atualizamos seu centróide.\n",
    "3. <b>TIRAR DISTANCIA</b><br>\n",
    "    &ensp; Para tirar as distancias de cada amostra até os centróides, selecionamos todas as <em>rows</em> do <em>dataframe</em> e, uma a uma, utilizamos a função myeuclidean() para realizar o cálculo.\n",
    "4. <b>DEFINIFIR GRUPOS</b><br>\n",
    "    &ensp; Tiradas as distâncias, com a combinação de funções index() e min(), determinamos qual o centróide mais próximo, para com isso, rotular corretamente o cluster adequado para a amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mykmeans(k, data, n_iterations):\n",
    "    for i in range(0, k): # initialize centroids with k first rows\n",
    "        data.loc[i, 'cluster_id'] = int(i)\n",
    "    for j in range(0, n_iterations): # run n_iteration times\n",
    "        for i in range(0, k): # (re)calc centroids\n",
    "            this = data[data.cluster_id == i] # capture all samples from centroid i\n",
    "            centroid.loc[i] = {'d1':this.d1.mean(), 'd2':this.d2.mean(), 'cluster_id':i} # atualizing centroid i\n",
    "        for index_data, row_data in data.iterrows(): # for each sample\n",
    "            calculated_distance = []\n",
    "            for index_centroid, row_centroid in centroid.iterrows(): # checks each sample and takes the distances\n",
    "                calculated_distance.append(myeuclidean(row_data, row_centroid))\n",
    "            row_data.loc['cluster_id'] = calculated_distance.index(min(calculated_distance)) # saves the shortest distance\n",
    "            data.iloc[index_data] = row_data # sends it back to the dataframe\n",
    "            \n",
    "    return data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo Single-Link\n",
    "\n",
    "Para a implementação da função de agrupamento Single-Link, o grupo recorreu a material de apoio disponibilizado na internet, checando sua implementação em diversos sites e até observando algoritmos otimizados de grandes bibliotecas confiáveis, como a SciPy. O grupo também informa que, mesmo tendo todo esse material disponível, para fins didáticos, tentou seguir com sua própria implementação adaptada desses materiais. Sendo assim, vamos à descrição do algoritmo.<br>\n",
    "1. <b>INICIALIZAÇÃO</b><br>\n",
    "    &emsp; Para a inicialização do algoritmo, declaramos cada amostra disponível no <em>dataframe</em> como um cluster, e jogamos todos na lista de clusters.\n",
    "2. <b>SIMULAR MATRIZ DE DISTÂNCIAS</b><br>\n",
    "    &emsp; O próximo passo, repetindo para cada cluster até obtivermos o número desejado k de clusters final, é determinar o par de amostras, de clusters diferentes, que têm a menor distância entre si. Isso é feito comparando todas as amostras numa matriz de distâncias de tamanho N, sendo N o número de amostras total. <br>É trivial dizer que a complexidade desse algoritmo é elevada nesta implementação, o que acabou trazendo dificuldades para o grupo em sua execução mais à frente no relatório.\n",
    "3. <b>ATRIBUIÇÕES E REMOÇÕES</b><br>\n",
    "    &emsp; Determinada a matriz de distância, pegamos o par de amostrar com menor distância entre si, se unimos seus clusters, até termos o numero total k desejado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysinglelink(data, kmin, kmax):\n",
    "    # initialization of clusters at first (every point is a cluster)\n",
    "    clusters=[]\n",
    "    ret_clusters=[]\n",
    "    for index, row in data.iterrows():\n",
    "        clusters.append([row]) # initialize with a cluster for every sample\n",
    "    while (len(clusters) > kmin): # check for n_cluster == k\n",
    "        menor_dist = clust_increment = clust_merge = math.inf # set as infinite\n",
    "        for cluster_id, cluster in enumerate(clusters[:len(clusters)]): # cicle though every cluster group \n",
    "            for sample_id, sample in enumerate(cluster): # cicle through every data in cluster group 1\n",
    "                for cluster_merge_id, cluster_merge in enumerate(clusters[(cluster_id+1):]): # cicle through every cluster group besides the first one\n",
    "                    for another_sample_id, another_sample in enumerate(cluster_merge): # cicle through every data in cluster group 1\n",
    "                        if myeuclidean(sample, another_sample) < menor_dist: # check for distance\n",
    "                            menor_dist = myeuclidean(sample, another_sample) # saves the distance if less than now\n",
    "                            clust_increment = cluster_id # saves the cluster to extend\n",
    "                            clust_merge = cluster_merge_id+cluster_id+1 # saves the cluster for removal\n",
    "        clusters[clust_increment].extend(clusters[clust_merge]) # merge cluster's copy\n",
    "        clusters.pop(clust_merge) # removes merged cluster's original copy\n",
    "        if len(clusters) <= kmax: # saves multiple copies from kmax to kmin\n",
    "            ret_clusters.append(clusters.copy()) \n",
    "    return(ret_clusters); # returns formed clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo Average-Link\n",
    "\n",
    "Para a implementação do algoritmo Average-Link, utilizamos da mesma estrutura feita para o Single-Link, mudando apenas suas métricas.<br>\n",
    "1. <b>INICIALIZAÇÃO</b><br>\n",
    "    &emsp; Para a inicialização do algoritmo, declaramos cada amostra disponível no <em>dataframe</em> como um cluster, e jogamos todos na lista de clusters.\n",
    "2. <b>TIRAR A DISTÃNCIA MÉDIA ENTRE CLUSTERS</b><br>\n",
    "    &emsp; Agora, diferentemente da implementação do Single-Link, nós iteramos para cada combinação de par de centróides, e novamente para todas as possibilidades de combinações de amostras entre esses mesmos clusters, salvando a soma total de distâncias em uma variável e o número total de distâncias calculadas. Com esses dados, podemos tirar a média de distância entre 2 clusters, para então, calcularmos qual a menor média de distâncias na iteração, e unir os clusteres correspondentes à essa união. <br>Novamente, é trivial dizer que a complexidade desse algoritmo é elevada nesta implementação, o que acabou trazendo dificuldades para o grupo em sua execução mais à frente no relatório.\n",
    "3. <b>ATRIBUIÇÕES E REMOÇÕES</b><br>\n",
    "    &emsp; Determinada a matriz de distância, pegamos o par de amostrar com menor distância entre si, se unimos seus clusters, até termos o numero total k desejado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myaveragelink(data, kmin, kmax):\n",
    "    # initialization of clusters at first (every point is a cluster)\n",
    "    clusters=[]\n",
    "    ret_clusters=[]\n",
    "    for index, row in data.iterrows():\n",
    "        clusters.append([row]) # initialize with a cluster for every sample\n",
    "    while (len(clusters) > kmin): # check for n_cluster == k\n",
    "        menor_dist = clust_increment = clust_merge = math.inf # set as infinite\n",
    "        for cluster_id, cluster in enumerate(clusters[:len(clusters)]): # cicle though every cluster group \n",
    "            for cluster_merge_id, cluster_merge in enumerate(clusters[(cluster_id+1):]): # cicle through every cluster group besides the first one\n",
    "                total_distance = 0 # reset sum of distances\n",
    "                n_distances = 0 # reset num of distances\n",
    "                for sample_id, sample in enumerate(cluster): # cicle through every data in cluster group 1\n",
    "                    for another_sample_id, another_sample in enumerate(cluster_merge): # cicle through every data in cluster group 1\n",
    "                        total_distance += myeuclidean(sample, another_sample) # increments total distance\n",
    "                        n_distances += 1 # increments num of distances\n",
    "                avg_dist = total_distance / n_distances # gets the average distance of 2 clusters\n",
    "                if avg_dist < menor_dist: # checks for new menor_dist\n",
    "                    menor_dist = avg_dist # saves the distance if less than now\n",
    "                    clust_increment = cluster_id # saves the cluster to extend\n",
    "                    clust_merge = cluster_merge_id+cluster_id+1 # saves the cluster for removal\n",
    "        clusters[clust_increment].extend(clusters[clust_merge]) # merge cluster's copy\n",
    "        clusters.pop(clust_merge) # removes merged cluster's original copy\n",
    "        if len(clusters) <= kmax: # saves multiple copies from kmax to kmin\n",
    "            ret_clusters.append(clusters.copy()) \n",
    "    return(ret_clusters); # returns formed clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodando os algoritmos\n",
    "\n",
    "### Primeiramente, o K-Médias rodando o primeiro dataset (c2ds1-2sp.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3339a1bd7e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmanipulated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# adding a column for cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcentroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'd1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# setting up clusters dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mkmeans1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmykmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanipulated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# salva o resultado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mkmeans1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gist_rainbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9d64a15ad689>\u001b[0m in \u001b[0;36mmykmeans\u001b[0;34m(k, data, n_iterations)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcalculated_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex_centroid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_centroid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# checks each sample and takes the distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mcalculated_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyeuclidean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_centroid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mrow_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculated_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculated_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# saves the shortest distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_data\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_data\u001b[0m \u001b[0;31m# sends it back to the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-49c90ae8d6a0>\u001b[0m in \u001b[0;36mmyeuclidean\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# funcção de distância euclidiana\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmyeuclidean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "# escolha aqui o valor de k e o numero de iterações\n",
    "k_list = [2, 3, 4, 5]\n",
    "n_iterations = 10\n",
    "\n",
    "kmeans1 = []\n",
    "for k in k_list:\n",
    "    manipulated_data = dataset1.copy() # setting up manipulated dataset for use\n",
    "    manipulated_data['cluster_id'] = None # adding a column for cluster\n",
    "    centroid = DataFrame(columns = ['d1', 'd2', 'cluster_id']) # setting up clusters dataframe\n",
    "    kmeans1.append(mykmeans(k, manipulated_data, n_iterations)) # salva o resultado\n",
    "\n",
    "    kmeans1[-1].plot.scatter('d1', 'd2', c='cluster_id', colormap='gist_rainbow')\n",
    "    plt.suptitle(\"K-Means for dataset 1 (c2ds1-2sp) with k = %d\" % k)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/thaina/Downloads/kmeans\"\n",
    "i = 2\n",
    "for cluster in kmeans1:\n",
    "    view = cluster.copy()\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos resultados, K-Médias(c2ds1-2sp):\n",
    "&emsp; Como podemos notar nas imagens acima, um algoritimo de agrupamento baseado em centro não é eficaz para separar adequadamente amostras como as do dataset1, dado que as amostras são bem decentralizadas e sequênciais.<br>\n",
    "&emsp; Podemos concluir que o resultado obtido foi longe do ideal, resultando apenas em uma \"setorização\" da imagem, dividindo-a em partes praticamente siméticas, porém sem nenhum valor embutido para análise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Médias rodando o segundo dataset (c2ds3-2g.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha aqui o valor de k e o numero de iterações\n",
    "k_list = [2, 3, 4, 5]\n",
    "n_iterations = 10\n",
    "\n",
    "kmeans2 = []\n",
    "for k in k_list:\n",
    "    manipulated_data = dataset2.copy() # setting up manipulated dataset for use\n",
    "    manipulated_data['cluster_id'] = None # adding a column for cluster\n",
    "    centroid = DataFrame(columns = ['d1', 'd2', 'cluster_id']) # setting up clusters dataframe\n",
    "    kmeans2.append(mykmeans(k, manipulated_data, n_iterations)) # salva o resultado\n",
    "\n",
    "    kmeans2[-1].plot.scatter('d1', 'd2', c='cluster_id', colormap='gist_rainbow')\n",
    "    plt.suptitle(\"Dataset 2 (c2ds3-2g) with k = %d\" % k)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/thaina/Downloads/kmeans/\"\n",
    "i = 2\n",
    "for cluster in kmeans2:\n",
    "    view = cluster.copy()\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos resultados, K-Médias(c2ds3-2g):\n",
    "&emsp; Para o dataset utilizado neste caso, agora sim temos um ótimo candidado para utilização de algoritmos de agrupamento baseados em centros. Podemos notar que o algoritmos separou com perfeição os dados utilizando k = 2, detectando os dois grandes centros de aglomeração de amostras e separando-os minuciosamente; Além de fazer uma boa separação para k = 3, detectando os dois mesmos grandes centros, mas separando também, uma \"área de colisão\", região onde os dados, apesar de estarem divididos entre os 2 grandes centros, têm um grau de semelhança considerável.<br>\n",
    "&emsp; Para k's maiores que 3, os agrupamentos parecem começar a ficar distorcidos e carecer de informação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Médias rodando o terceiro e último dataset (monkey.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha aqui o valor de k e o numero de iterações\n",
    "k_list = [5, 6, 7, 8,  9, 10, 11, 12]\n",
    "n_iterations = 10\n",
    "\n",
    "kmeans3 = []\n",
    "for k in k_list:\n",
    "    manipulated_data = dataset3.copy() # setting up manipulated dataset for use\n",
    "    manipulated_data['cluster_id'] = None # adding a column for cluster\n",
    "    centroid = DataFrame(columns = ['d1', 'd2', 'cluster_id']) # setting up clusters dataframe\n",
    "    kmeans3.append(mykmeans(k, manipulated_data, n_iterations)) # salva o resultado\n",
    "    \n",
    "    kmeans3[-1].plot.scatter('d1', 'd2', c='cluster_id', colormap='gist_rainbow')\n",
    "    plt.suptitle(\"Dataset 3 (monkey) with k = %d\" % k)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/thaina/Downloads/kmeans/d3/\"\n",
    "i = 5\n",
    "for cluster in kmeans3:\n",
    "    view = cluster.copy()\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos resultados, K-Médias(monkey):\n",
    "&emsp; Vamos agora para a análise dos agrupamentos formados no dataset 3(monkey).<br>\n",
    "&emsp; Para os agrupamentos formados com k entre 5 e 6, notamos que não houve efetividade do algoritmo para fazer a clusterização. É notável que, como no caso do dataset 1, o algoritmo apenas \"setorizou\" a imagem, dividindo-a com \"linhas imaginárias\".<br>\n",
    "&emsp; No restante dos agrupamentos, pode-se perceber uma sutil melhora na clusterização, onde o algoritimo demonstra a capacidade de reconhecer parcialmente as divisões menos espaçadas; Sendo elas as \"orelhas\" do macaco, seus \"olhos\" e sobrancelhas. Traços decentralizados como o contorno da \"cabeça\" e \"boca\" tiveram pouca efetivade nos agrupamentos gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, o Single-Link rodando o primeiro dataset (c2ds1-2sp.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha o valor de k aqui\n",
    "kmin = 2\n",
    "kmax = 5\n",
    "\n",
    "# color generating\n",
    "colors = []\n",
    "for i in range(kmax):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "# plot generating for dataset 1\n",
    "view = dataset1\n",
    "#clusters_s1 = mysinglelink(view, kmin, kmax)\n",
    "for k_clusters_index, k_clusters in enumerate(clusters_s1):\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample_index, sample in enumerate(cluster):\n",
    "            plt.plot([sample['d1']], [sample['d2']], marker='o', markersize=3, color=colors[cluster_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/debora/Documents/IA/Trab/singlelink/d1/\"\n",
    "i = 5\n",
    "\n",
    "view = DataFrame(columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "test = clusters_s1.copy()\n",
    "\n",
    "single1 = []\n",
    "for k_clusters_index, k_clusters in enumerate(test):\n",
    "    listing = []\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample in cluster:\n",
    "            sample['cluster_id'] = cluster_index\n",
    "        listing += cluster\n",
    "    view = DataFrame(listing, columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "    view.sort_index(inplace=True)\n",
    "    view.drop_duplicates(keep='first', inplace=True)\n",
    "    single1.append([i, view])\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos resultados, Single-Link(c2ds1-2sp):\n",
    "&emsp; Após logo o primeiro exemplo deste <em>dataset</em>, com k = 2, podemos perceber a efetividade do algoritmo Single-Link para dados ditos \"encadeados\", separando perfeitamente as duas \"linhas\" formadas pelos dados.<br>\n",
    "&emsp; Podemos também perceber sua constância na geração dos clusters para k's maiores que 2, onde, geralmente, os clusters que não os dois encadeamentos principais, normalmente constam de 1 dado apenas (o dado que têm a maior distância para um próximo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Link rodando o segundo dataset (c2ds3-2g.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha o valor de k aqui\n",
    "kmin = 2\n",
    "kmax = 5\n",
    "\n",
    "# color generating\n",
    "colors = []\n",
    "for i in range(kmax):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "# plot generating for dataset 1\n",
    "view = dataset2\n",
    "#clusters_s2 = mysinglelink(view, kmin, kmax)\n",
    "for k_clusters_index, k_clusters in enumerate(clusters_s2):\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample_index, sample in enumerate(cluster):\n",
    "            plt.plot([sample['d1']], [sample['d2']], marker='o', markersize=3, color=colors[cluster_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/debora/Documents/IA/Trab/singlelink/d2/\"\n",
    "i = 5\n",
    "\n",
    "view = DataFrame(columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "test = clusters_s2.copy()\n",
    "\n",
    "single2 = []\n",
    "for k_clusters_index, k_clusters in enumerate(test):\n",
    "    listing = []\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample in cluster:\n",
    "            sample['cluster_id'] = cluster_index\n",
    "        listing += cluster\n",
    "    view = DataFrame(listing, columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "    view.sort_index(inplace=True)\n",
    "    view.drop_duplicates(keep='first', inplace=True)\n",
    "    single2.append([i, view])\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos resultados, Single-Link(c2ds3-2g):\n",
    "&emsp; Com este exemplo, podemos perceber agora a inefetividade do algoritmo Single-Link para dados centralizados, que demonstra incapacidade de agrupar praticamente todos os casos de maneira eficaz.<br>\n",
    "&emsp; Como no dataset anterior, normalmente dados considerados \"outlyers\" pelo algorimto são tratados como clusters unitários, sendo compostos pelos dados mais afastados de um vizinho qualquer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Link rodando o terceiro e último dataset (monkey.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha o valor de k aqui\n",
    "kmin = 5\n",
    "kmax = 12\n",
    "\n",
    "single1 = []\n",
    "# color generating\n",
    "colors = []\n",
    "for i in range(kmax):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "# plot generating for dataset 1\n",
    "view = dataset3\n",
    "clusters = mysinglelink(view, kmin, kmax)\n",
    "for k_clusters_index, k_clusters in enumerate(clusters):\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample_index, sample in enumerate(cluster):\n",
    "            plt.plot([sample['d1']], [sample['d2']], marker='o', markersize=3, color=colors[cluster_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por fim, o Average-Link rodando o primeiro dataset (c2ds1-2sp.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha o valor de k aqui\n",
    "kmin = 2\n",
    "kmax = 5\n",
    "\n",
    "# color generating\n",
    "colors = []\n",
    "for i in range(kmax):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "# plot generating for dataset 1\n",
    "view = dataset1\n",
    "clusters_a1 = myaveragelink(view, kmin, kmax)\n",
    "for k_clusters_index, k_clusters in enumerate(clusters_a1):\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample_index, sample in enumerate(cluster):\n",
    "            plt.plot([sample['d1']], [sample['d2']], marker='o', markersize=3, color=colors[cluster_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/debora/Documents/IA/Trab/averagelink/d1/\"\n",
    "i = 5\n",
    "\n",
    "view = DataFrame(columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "test = clusters_a1.copy()\n",
    "\n",
    "average1 = []\n",
    "for k_clusters_index, k_clusters in enumerate(test):\n",
    "    listing = []\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample in cluster:\n",
    "            sample['cluster_id'] = cluster_index\n",
    "        listing += cluster\n",
    "    view = DataFrame(listing, columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "    view.sort_index(inplace=True)\n",
    "    view.drop_duplicates(keep='first', inplace=True) \n",
    "    average1.append([i, view])\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average-Link rodando o segundo dataset (c2ds3-2g.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha o valor de k aqui\n",
    "kmin = 2\n",
    "kmax = 5\n",
    "\n",
    "average2 = []\n",
    "# color generating\n",
    "colors = []\n",
    "for i in range(kmax):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "# plot generating for dataset 2\n",
    "view = dataset2\n",
    "clusters_a2 = myaveragelink(view, kmin, kmax)\n",
    "for k_clusters_index, k_clusters in enumerate(clusters_a2):\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample_index, sample in enumerate(cluster):\n",
    "            plt.plot([sample['d1']], [sample['d2']], marker='o', markersize=3, color=colors[cluster_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/debora/Documents/IA/Trab/averagelink/d2/\"\n",
    "i = 5\n",
    "\n",
    "view = DataFrame(columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "test = clusters_a2.copy()\n",
    "\n",
    "average2 = []\n",
    "for k_clusters_index, k_clusters in enumerate(test):\n",
    "    listing = []\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample in cluster:\n",
    "            sample['cluster_id'] = cluster_index\n",
    "        listing += cluster\n",
    "    view = DataFrame(listing, columns = ['sample_label','d1', 'd2', 'cluster_id'])\n",
    "    view.sort_index(inplace=True)\n",
    "    view.drop_duplicates(keep='first', inplace=True) \n",
    "    average2.append([i, view])\n",
    "    view.to_csv(path_or_buf=filepath+str(i)+\".clu\", header=False , columns=['sample_label', 'cluster_id'], sep=\"\\t\", index=False)\n",
    "    i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average-Link rodando o terceiro e último dataset (monkey.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolha o valor de k aqui\n",
    "kmin = 5\n",
    "kmax = 12\n",
    "\n",
    "average3 = []\n",
    "# color generating\n",
    "colors = []\n",
    "for i in range(kmax):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "# plot generating for dataset 3\n",
    "view = dataset3\n",
    "clusters = myaveragelink(view, kmin, kmax)\n",
    "for k_clusters_index, k_clusters in enumerate(clusters):\n",
    "    for cluster_index, cluster in enumerate(k_clusters):\n",
    "        for sample_index, sample in enumerate(cluster):\n",
    "            plt.plot([sample['d1']], [sample['d2']], marker='o', markersize=3, color=colors[cluster_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando o Índice Rand Ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected1 = read_csv(\"datasets/c2ds1-2spReal.clu\", sep=\"\\t\", header=None) # Lê o arquivo de dados\n",
    "expected2 = read_csv(\"datasets/c2ds3-2gReal.clu\", sep=\"\\t\", header=None) # Lê o arquivo de dados\n",
    "expected3 = read_csv(\"datasets/monkeyReal1.clu\", sep=\"\\t\", header=None) # Lê o arquivo de dados\n",
    "\n",
    "test = read_csv(\"singlelink/d3/12.clu\", sep=\"\\t\", header=None)\n",
    "\n",
    "display(adjusted_rand_score(test[1], expected3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
